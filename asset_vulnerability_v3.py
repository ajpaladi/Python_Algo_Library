import warnings
import pandas as pd
import geopandas as geo
import numpy as np
from tqdm import tqdm
import os
import plotly.express as px
import uuid
import osmnx as ox
from keplergl import KeplerGl
from shapely.geometry import Point, Polygon, MultiPolygon, MultiPolygon, MultiPoint
from geopy.geocoders import Nominatim
locator = Nominatim(user_agent="myGeocoder")
import requests

warnings.simplefilter(action='ignore', category=FutureWarning)

#osmnx config timeout
timeout = 3000
ox.config(timeout=timeout)

nws_region = pd.read_csv('data_dir/nws_region_codes.csv')
state_ab = pd.read_csv('data_dir/state_abbv.csv')
counties = geo.read_file('data_dir/c_13se22/c_13se22.shp')

#### create dictionary from NWS alerts, add geocodes ####

w_d = {'state': [], 'area': [], 'event':[], 'sent':[], 'onset':[], 'ends':[], 'geocode': [], 'severity': [],
       'certainty':[], 'urgency':[], 'headline': [], 'descr': []}

for ab in state_ab.state_abbv:
    url = f'https://api.weather.gov/alerts/active?area={ab}'
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
    else:
        print(f'Error: {response.status_code}')
       
    for i in data['features']:
        #print(i['properties'])
        if i['properties']['severity'] == 'Severe':
            state = ab
            w_d['state'].append(state)
            area = i['properties']['areaDesc']
            w_d['area'].append(area)
            event = i['properties']['event']
            w_d['event'].append(event)
            sent = i['properties']['sent']
            w_d['sent'].append(sent)
            onset = i['properties']['onset']
            w_d['onset'].append(onset)
            ends = i['properties']['ends']
            w_d['ends'].append(ends)
            geocode = i['properties']['geocode']['SAME']
            w_d['geocode'].append(geocode)
            severity = i['properties']['severity']
            w_d['severity'].append(severity)
            urgency = i['properties']['urgency']
            w_d['urgency'].append(urgency)
            certainty = i['properties']['certainty']
            w_d['certainty'].append(certainty)
            headline = i['properties']['headline']
            w_d['headline'].append(headline)
            descr = i['properties']['description']
            w_d['descr'].append(descr)
            
report = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in w_d.items() ]))
report['sent'] = pd.to_datetime(report['sent'], utc = True)
report['onset'] = pd.to_datetime(report['onset'], utc = True)
report['ends'] = pd.to_datetime(report['ends'], utc = True)
report.to_csv('report.csv')

#take all geocodes associated with report, and match then to FIPS geocodes from Counties shp


affected = []

for i in report['geocode'].iloc[0:]:
    no_zeros = [zero.lstrip('0') for zero in i]
    for nzero in no_zeros:
        affected.append(nzero)
        
affected_nd = list(set(affected))
affected_nd.sort(reverse = False)


affected_counties = geo.GeoDataFrame()

try:
    for f in affected_nd:
        counties['FIPS'] = counties['FIPS'].str.lstrip('0')
        county = counties[(counties['FIPS'] == f)]
        affected_counties = affected_counties.append(county)
except FutureWarning:
    pass

### prep the data, perform a table merge on counties and report

report = report.explode('geocode')
report = report.drop_duplicates(subset = ['geocode', 'event', 'area'])
report['geocode'] = report['geocode'].str.lstrip('0')
merged_affected = pd.merge(report, affected_counties, left_on='geocode', right_on='FIPS', how='left')
geo_merge = geo.GeoDataFrame(merged_affected, geometry='geometry')
geo_merge.to_csv('geo_merge.csv')
geo_merge = geo_merge.to_crs({'init': 'epsg:4326'})

### using the unique geocodes from the report, pull osm geometries (power : plant) ###

affected_utilities = geo.GeoDataFrame()

for code in geo_merge['geocode'].unique():
    geo_u = geo_merge[(geo_merge.geocode == code)]
    geom = geo_u.geometry.iloc[0]
    tags = {'power': 'plant'}        
    osm_pull = ox.geometries.geometries_from_polygon(geom, tags)
    osm_pull.reset_index(inplace = True)
    affected_utilities = pd.concat([affected_utilities, osm_pull])
    affected_utilities.to_csv('affected_utilities.csv')

#select only a few rows
v_utils = affected_utilities[['geometry', 'name', 'operator', 'plant:source', 'plant:method',
                              'plant:output:electricity',  'source', 'osmid', 'element_type', 'power']]

#perform a spatial merge on the returned osm_boundaries and the merged file, delete dupes

prod = geo.GeoDataFrame(v_utils, geometry='geometry')
merged = geo.sjoin(prod, geo_merge, how='inner', op='intersects')
final = merged.drop_duplicates(subset = 'osmid')
final['centroid'] = final['geometry'].centroid
final['descr'] = final['descr'].str.wrap(500)
final = final.fillna('NA')

#more cleaning
final['certainty_n'] = ''
final['urgency_n'] = ''
final['certainty_n'] = final['certainty'].apply(lambda x: 3 if x == 'Observed' else (2 if x == 'Likely' else (1 if x == 'Possible' else None)))
final['urgency_n'] = final['urgency'].apply(lambda x: 3 if x == 'Immediate' else (2 if x == 'Expected' else (1 if x == 'Future' else None)))
final.to_csv('final.csv')
final.head()

# query only major threats
query = final[(final.certainty == 'Observed') & (final.urgency == 'Immediate')]

##getting forcast for each grid associated with 'query'
#NEXT LEVEL BABYYYYYYY

f_df = geo.GeoDataFrame()

for x, y in zip(query['centroid'].x, query['centroid'].y):
    url = f'https://api.weather.gov/points/{y},{x}'
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        forecast = data['properties']['forecast']
    else:
        print(f'Error: {response.status_code}')
    url_forecast = forecast
    response = requests.get(url_forecast)
    if response.status_code == 200:
        data_forecast = response.json()
        detailed_forecast = data_forecast['properties']['periods'][0]['detailedForecast']
        print(detailed_forecast)
        geojson = data_forecast['geometry']['coordinates']
        polygon = Polygon(geojson[0])
        gdf = geo.GeoDataFrame(geometry=[polygon])
        f_df = f_df.append(gdf)

f_df['centroid'] = f_df['geometry'].centroid
f_df.to_csv('fdf.csv')
        
##### data_forecast['properties']['periods'][0] ########
##### data_forecast['properties']['periods'][0]['detailedForecast']#####
