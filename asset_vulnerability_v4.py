import warnings
import pandas as pd
import geopandas as geo
import numpy as np
from tqdm import tqdm
import os
import plotly.express as px
import uuid
import osmnx as ox
from keplergl import KeplerGl
from shapely.geometry import Point, Polygon, MultiPolygon, MultiPolygon, MultiPoint
from geopy.geocoders import Nominatim
locator = Nominatim(user_agent="myGeocoder")
import requests

warnings.simplefilter(action='ignore', category=FutureWarning)

#osmnx config timeout
timeout = 3000
ox.config(timeout=timeout)

nws_region = pd.read_csv('data_dir/nws_region_codes.csv')
state_ab = pd.read_csv('data_dir/state_abbv.csv')
counties = geo.read_file('data_dir/c_13se22/c_13se22.shp')

#### create dictionary from NWS alerts, add geocodes ####

w_d = {'state': [], 'area': [], 'event':[], 'sent':[], 'onset':[], 'ends':[], 'geocode': [], 'severity': [],
       'certainty':[], 'urgency':[], 'headline': [], 'descr': []}

for ab in state_ab.state_abbv:
    url = f'https://api.weather.gov/alerts/active?area={ab}'
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
    else:
        print(f'Error: {response.status_code}')
       
    for i in data['features']:
        #print(i['properties'])
        if i['properties']['severity'] == 'Severe':
            state = ab
            w_d['state'].append(state)
            area = i['properties']['areaDesc']
            w_d['area'].append(area)
            event = i['properties']['event']
            w_d['event'].append(event)
            sent = i['properties']['sent']
            w_d['sent'].append(sent)
            onset = i['properties']['onset']
            w_d['onset'].append(onset)
            ends = i['properties']['ends']
            w_d['ends'].append(ends)
            geocode = i['properties']['geocode']['SAME']
            w_d['geocode'].append(geocode)
            severity = i['properties']['severity']
            w_d['severity'].append(severity)
            urgency = i['properties']['urgency']
            w_d['urgency'].append(urgency)
            certainty = i['properties']['certainty']
            w_d['certainty'].append(certainty)
            headline = i['properties']['headline']
            w_d['headline'].append(headline)
            descr = i['properties']['description']
            w_d['descr'].append(descr)
            
report = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in w_d.items() ]))
report['sent'] = pd.to_datetime(report['sent'], utc = True)
report['onset'] = pd.to_datetime(report['onset'], utc = True)
report['ends'] = pd.to_datetime(report['ends'], utc = True)
report.to_csv('report.csv')

#take all geocodes associated with report, and match then to FIPS geocodes from Counties shp


affected = []

for i in report['geocode'].iloc[0:]:
    no_zeros = [zero.lstrip('0') for zero in i]
    for nzero in no_zeros:
        affected.append(nzero)
        
affected_nd = list(set(affected))
affected_nd.sort(reverse = False)


affected_counties = geo.GeoDataFrame()

try:
    for f in affected_nd:
        counties['FIPS'] = counties['FIPS'].str.lstrip('0')
        county = counties[(counties['FIPS'] == f)]
        affected_counties = affected_counties.append(county)
except FutureWarning:
    pass

### prep the data, perform a table merge on counties and report

report = report.explode('geocode')
report = report.drop_duplicates(subset = ['geocode', 'event', 'area'])
report['geocode'] = report['geocode'].str.lstrip('0')
merged_affected = pd.merge(report, affected_counties, left_on='geocode', right_on='FIPS', how='left')
geo_merge = geo.GeoDataFrame(merged_affected, geometry='geometry')
geo_merge.to_csv('geo_merge.csv')
geo_merge = geo_merge.to_crs({'init': 'epsg:4326'})

### using the unique geocodes from the report, pull osm geometries (power : plant) ###

affected_utilities = geo.GeoDataFrame()

for code in geo_merge['geocode'].unique():
    geo_u = geo_merge[(geo_merge.geocode == code)]
    geom = geo_u.geometry.iloc[0]
    tags = {'power': 'plant'}        
    osm_pull = ox.geometries.geometries_from_polygon(geom, tags)
    osm_pull.reset_index(inplace = True)
    affected_utilities = pd.concat([affected_utilities, osm_pull])
    affected_utilities.to_csv('affected_utilities.csv')

#select only a few rows
v_utils = affected_utilities[['geometry', 'name', 'operator', 'plant:source', 'plant:method',
                              'plant:output:electricity',  'source', 'osmid', 'element_type', 'power']]

#perform a spatial merge on the returned osm_boundaries and the merged file, delete dupes

prod = geo.GeoDataFrame(v_utils, geometry='geometry')
merged = geo.sjoin(prod, geo_merge, how='inner', op='intersects')
final = merged.drop_duplicates(subset = 'osmid')    # MAY NOT HAVE WANTED TO DO THIS ??
final['centroid'] = final['geometry'].centroid
final['descr'] = final['descr'].str.wrap(500)
final = final.fillna('NA')

#more cleaning
final['certainty_n'] = ''
final['urgency_n'] = ''
final['certainty_n'] = final['certainty'].apply(lambda x: 3 if x == 'Observed' else (2 if x == 'Likely' else (1 if x == 'Possible' else None)))
final['urgency_n'] = final['urgency'].apply(lambda x: 3 if x == 'Immediate' else (2 if x == 'Expected' else (1 if x == 'Future' else None)))
final.to_csv('final.csv')
final.head()

# query only major threats
query = final[(final.certainty == 'Observed') & (final.urgency == 'Immediate')]

## MORE GRANULAR
## getting forcast for each grid associated with 'query'
## NEXT LEVEL BABYYYYYYY ~ something I have yet to do in python (list to columns in a df)

f_df = geo.GeoDataFrame() #final geodataframe
d_forecast = []
s_forecast = []
temp = []
w_speed = []
chance_of_precip = []
elev = []

for x, y in zip(query['centroid'].x, query['centroid'].y):
    url = f'https://api.weather.gov/points/{y},{x}'
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        forecast = data['properties']['forecast']
    else:
        print(f'Error: {response.status_code}')
    url_forecast = forecast
    response = requests.get(url_forecast)
    if response.status_code == 200:
        data_forecast = response.json()
        elevation = data_forecast['properties']['elevation']['value']
        detailed_forecast = data_forecast['properties']['periods'][0]['detailedForecast']
        short_forecast = data_forecast['properties']['periods'][0]['shortForecast']
        temperature = data_forecast['properties']['periods'][0]['temperature']
        wind_speed = data_forecast['properties']['periods'][0]['windSpeed'][:-4]
        per_precip = data_forecast['properties']['periods'][0]['probabilityOfPrecipitation']['value']
        d_forecast.append(detailed_forecast)
        s_forecast.append(short_forecast)
        temp.append(temperature)
        w_speed.append(wind_speed)
        chance_of_precip.append(per_precip)
        elev.append(elevation)
        geojson = data_forecast['geometry']['coordinates']
        polygon = Polygon(geojson[0])
        gdf = geo.GeoDataFrame(geometry=[polygon], crs='EPSG:4326')
        f_df = f_df.append(gdf)

#adding lists above to df, data cleaning

f_df['centroid'] = f_df['geometry'].centroid
f_df['detailed_forecast'] = d_forecast
f_df['short_forecast'] = s_forecast
f_df['temperature'] = temp
f_df['wind_speed'] = w_speed
f_df['chance_of_precipitation'] = chance_of_precip
f_df['elevation_m'] = elev
f_df['elevation_ft'] = f_df['elevation_m'] * 3.28084
f_df.fillna(0, inplace = True)
f_df = f_df.to_crs(4326)
f_df.to_csv('fdf.csv')

#spatial_join the f_df grid data on the osmnx query

query.drop(columns = 'index_right', inplace = True)
join_n = query.sjoin_nearest(f_df, how='inner', lsuffix='left', rsuffix='right')
join_n.sort_values(by = 'name', inplace = True)
join_n.drop_duplicates(subset = ['name', 'osmid'], inplace = True) #### !!!!!!!!! maybe also implement above

#examples
'''
px.set_mapbox_access_token(open("mapbox_token.txt").read())
fig = px.density_mapbox(final, lat=final['centroid'].y, lon=final['centroid'].x, z='certainty_n', radius=15,
                        color_continuous_scale=px.colors.sequential.Inferno, zoom=3, hover_name = 'event',
                        mapbox_style="dark", title = 'Heatmap of Potentially Affected Assets and Certainty of Event')
fig.update_layout(
    width=1100, 
    height=600 
)
fig.show()
'''
###############################

'''
px.set_mapbox_access_token(open("mapbox_token.txt").read())

fig = px.scatter_mapbox(final, lat=final['centroid'].y, lon=final['centroid'].x, hover_name = 'name', 
                        color_continuous_scale=px.colors.sequential.Turbo, mapbox_style="dark",
                        color = 'event', size= 'certainty_n', size_max=10, zoom=3, title= 'Event Type and Level of Certainty')
fig.update_layout(
    width=1100, 
    height=600 
)
fig.show()
'''
##############################

'''
px.set_mapbox_access_token(open("mapbox_token.txt").read())

fig = px.scatter_mapbox(query, lat=query['centroid'].y, lon=query['centroid'].x, hover_name = 'name', 
                        color_continuous_scale=px.colors.sequential.Turbo, mapbox_style="dark",
                        color = 'event', size= 'certainty_n', size_max=10, zoom=3, title= 'Events Happening Now')
fig.update_layout(
    width=1100, 
    height=600 
)
fig.show()
'''

###############################

'''
px.set_mapbox_access_token(open("mapbox_token.txt").read())

fig = px.scatter_mapbox(f_df, lat=f_df['centroid'].y, lon=f_df['centroid'].x, color = 'short_forecast', hover_name = 'detailed_forecast',
                        color_continuous_scale=px.colors.sequential.Turbo, mapbox_style="dark",
                        size_max=10, zoom=3, title= 'Events Happening Now')
fig.update_layout(
    width=1100, 
    height=600 
)
fig.show()
'''

###############################

'''
px.set_mapbox_access_token(open("mapbox_token.txt").read())

fig = px.scatter_mapbox(f_df, lat=f_df['centroid'].y, lon=f_df['centroid'].x, color = 'temperature',
                        color_continuous_scale=px.colors.sequential.Turbo, mapbox_style="light",
                        size_max=10, zoom=3, title= 'Events Happening Now')
fig.update_layout(
    width=1100, 
    height=600 
)
fig.show()
'''

